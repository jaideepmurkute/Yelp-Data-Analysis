{"cells":[{"cell_type":"code","execution_count":146,"metadata":{"execution":{"iopub.execute_input":"2024-03-13T06:34:59.136558Z","iopub.status.busy":"2024-03-13T06:34:59.135539Z","iopub.status.idle":"2024-03-13T06:34:59.146076Z","shell.execute_reply":"2024-03-13T06:34:59.144501Z","shell.execute_reply.started":"2024-03-13T06:34:59.136497Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'data_dir': '/kaggle/input/yelp-compressed-dataset'}\n"]}],"source":["import os\n","import json\n","import gc\n","\n","import numpy as np \n","import pandas as pd \n","import pyarrow as pa\n","import pyarrow.parquet as pq\n","\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n","\n","# -------------------------------------------------------\n","\n","config = {\n","            'data_dir': '/kaggle/input/yelp-compressed-dataset',\n","        }\n","\n","print(config)\n"]},{"cell_type":"code","execution_count":147,"metadata":{"execution":{"iopub.execute_input":"2024-03-13T06:34:59.186459Z","iopub.status.busy":"2024-03-13T06:34:59.185981Z","iopub.status.idle":"2024-03-13T06:34:59.236219Z","shell.execute_reply":"2024-03-13T06:34:59.234886Z","shell.execute_reply.started":"2024-03-13T06:34:59.186426Z"},"trusted":true},"outputs":[],"source":["def read_data_file(config, file_name=None, verbose=False):\n","    assert file_name is not None\n","\n","    filepath = os.path.join(config['data_dir'], file_name+'.parquet')\n","\n","    if verbose: print(f\"Reading file: {filepath}\")\n","    df = pq.read_table(filepath).to_pandas()\n","    if verbose: print(\"df.shape: \", df.shape)\n","\n","    return df\n","\n","def drop_inf_rows(df):\n","    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n","    df.dropna(inplace=True)\n","    \n","def filter_data(config, df, file_name=None, drop_cols=None, dropna=False, dropinf=False, verbose=False):\n","    \n","    if verbose: print(\"Before filtering shape: \", df.shape)\n","    \n","    if drop_cols is not None:\n","        if verbose: \n","            print(\"Before dropping columns: \", df.columns)\n","            print(\"Before dropping columns shape:\", df.shape)\n","        df.drop(drop_cols, axis=1, inplace=True)\n","        if verbose: \n","            print(\"After dropping columns: \", df.columns)\n","            print(\"After dropping columns shape:\", df.shape)\n","        \n","    # Filter out rows with NaN values\n","    if dropna:\n","        if verbose: print(\"Before dropping NaNs shape:\", df.shape)\n","        df.dropna(inplace=True)\n","        if verbose: print(\"After dropping NaNs shape:\", df.shape)\n","\n","    # Filter out rows with inf values\n","    if dropinf:\n","        if verbose: print(\"Before dropping Infs shape:\", df.shape)\n","        df.replace([np.inf, -np.inf], np.nan, inplace=True)\n","        df.dropna(inplace=True)\n","        if verbose: print(\"After dropping Infs shape:\", df.shape)\n","\n","    # Filter out ir-relevant data for current analysis\n","    assert file_name is not None\n","    if file_name == 'business':\n","        if verbose: print(\"Before filterin out non-restaurant data shape:\", df.shape)\n","        df = df[df['categories'].str.lower().str.contains('restaurants')]\n","        if verbose: print(\"After filterin out non-restaurant data shape:\", df.shape)\n","    \n","    if verbose: print(\"After filtering shape: \", df.shape)\n","\n","    return df\n","\n","\n","def handle_outliers_fn(df, cols, mode='drop', threshold=3, verbose=False):\n","    if verbose: print(\"Before outlier handling shape: \", df.shape)\n","    \n","    if mode == 'drop':\n","        print(\"Dropping outliers...\")\n","        for col in cols:\n","            z_scores = (df[col] - df[col].mean()) / df[col].std()\n","            df = df[z_scores.abs() < threshold]\n","    elif mode == 'clip':\n","        print(\"Clipping outliers...\")\n","        for col in cols:\n","            lower_bound = df[col].mean() - threshold * df[col].std()\n","            upper_bound = df[col].mean() + threshold * df[col].std()\n","            df[col] = df[col].clip(lower_bound, upper_bound)\n","    \n","    if verbose: print(\"After outlier handling shape: \", df.shape)\n","\n","    return df\n","\n","\n","def remove_duplicates(df, verbose=False):\n","    if verbose: print(\"Before removing duplicates shape: \", df.shape)\n","    df.drop_duplicates(inplace=True)\n","    if verbose: print(\"After removing duplicates shape: \", df.shape)\n","    \n","    return df\n","\n","\n","def convert_datetime_column(df, col_name):\n","    for idx, dt_str in enumerate(df[col_name]):\n","        try:\n","            df.loc[idx, col_name] = pd.to_datetime(dt_str, format='%Y-%m-%d %H:%M:%S')  # Or format='mixed'\n","        except ValueError:\n","            print(f\"Failed to convert datetime string at index {idx}: {dt_str}\")    \n","    return df\n","\n","\n","def encode_dates_fn(df, date_cols=None, verbose=False):\n","    if verbose: print(\"Before encoding data shape: \", df.shape)\n","    \n","    # If the date columns exist; and convert to the datetime format\n","    if date_cols is None:\n","        # infer date columns based on the names\n","        date_cols = [col for col in df.columns if 'date' in col.lower()]\n","    if len(date_cols) > 0:\n","        for col in date_cols:\n","            # format = '%Y-%m-%d %H:%M:%S'\n","            # format='mixed'\n","            df[col] = df[col].apply(lambda x: x.lstrip(', '))\n","            df[col] = df[col].apply(lambda x: x.strip(' '))\n","            df[col] = pd.to_datetime(df[col], format='%Y-%m-%d %H:%M:%S')\n","            # df = convert_datetime_column(df, col)\n","            \n","    if verbose: print(\"After encoding data shape: \", df.shape)\n","    \n","    return df\n","\n","\n","def get_column_descriptor_dict(config, df, id_cols=None, verbose=False):\n","    col_descriptor_dict = dict()\n","\n","    # create columns type descriptors - int_cols, float_cols, bool_cols, id_cols, numeric_cols, date_cols etc.\n","    if id_cols is None:\n","        col_descriptor_dict['id_cols'] = [col for col in df.columns if 'id' in col.lower()]\n","    col_descriptor_dict['int_cols'] = [col for col in df.columns if 'int' in str(df[col].dtype)]\n","    col_descriptor_dict['float_cols'] = [col for col in df.columns if 'float' in str(df[col].dtype)]\n","    col_descriptor_dict['nume_cols'] = col_descriptor_dict['int_cols'] + \\\n","                                        col_descriptor_dict['float_cols']\n","    col_descriptor_dict['non_nume_cols'] = [col for col in df.columns \\\n","                                            if col not in col_descriptor_dict['nume_cols']]\n","\n","    if verbose:\n","        for k, v in col_descriptor_dict.items():\n","            print(f\"{k}: \")\n","            print(f\"{v}\")\n","            print(\"-\"*30)\n","\n","    return col_descriptor_dict\n","\n","\n","def get_scaler(scaler_type):\n","    \"\"\"\n","    Returns a scikit-learn scaler object based on the specified scaler type.\n","\n","    Args:\n","        scaler_type (str): Type of scaler. Options are 'standard', 'min_max', 'robust'.  \n","\n","    Returns:\n","        sklearn.preprocessing.scaler: A scikit-learn scaler object.\n","    \"\"\"\n","\n","    if scaler_type == 'standard':\n","        scaler = StandardScaler()  # Standardize features (zero mean, unit variance)\n","    elif scaler_type == 'min_max':\n","        scaler = MinMaxScaler()  # Scale features to a given range (often 0 to 1)\n","    elif scaler_type == 'robust':\n","        scaler = RobustScaler()  # Scale features using statistics robust to outliers \n","    else:\n","        raise ValueError(f\"Invalid scaler_type: {scaler_type}\")  # Handle invalid input \n","    \n","    return scaler\n","\n","\n","def scale_data_fn(df, scaler_type_cols_map, scaler_type='standard', verbose=False):\n","    if verbose: print(\"Before scaling data shape: \", df.shape)\n","    \n","    for scaler_type, cols in scaler_type_cols_map.items():\n","        print(f\"Using {scaler_type} scaler for cols: {cols}\")\n","        scaler = get_scaler(scaler_type)\n","        for col in cols:\n","            df[[col]] = scaler.fit_transform(df[[col]])\n","        \n","    if verbose: print(\"After scaling data shape: \", df.shape)\n","    \n","    return df\n","\n","\n","def preprocess_data(config, df, encode_dates=False, date_cols=None, id_cols=None, \n","                    handle_outliers=False, \n","                    scale_data=False, cols_scaler_type_map=None, verbose=False):\n","    \n","    df = remove_duplicates(df, verbose=verbose)\n","    \n","    if encode_dates:\n","        df = encode_dates_fn(df, date_cols, verbose)\n","    \n","    col_descriptor_dict = get_column_descriptor_dict(config, df, id_cols=id_cols, \\\n","                                                     verbose=verbose)\n","    \n","    if handle_outliers:\n","        df = handle_outliers_fn(df, col_descriptor_dict['nume_cols'], \n","                                mode='drop', threshold=3, verbose=verbose)\n","    \n","    scaler_type_cols_map = {'standard': col_descriptor_dict['float_cols']}\n","    if scale_data and scaler_type_cols_map:\n","        print(\"scaling data....................\")\n","        df = scale_data_fn(df, scaler_type_cols_map, verbose=verbose)\n","    \n","    return df, col_descriptor_dict\n","    \n"]},{"cell_type":"code","execution_count":148,"metadata":{"execution":{"iopub.execute_input":"2024-03-13T06:34:59.240157Z","iopub.status.busy":"2024-03-13T06:34:59.238953Z","iopub.status.idle":"2024-03-13T06:35:03.872913Z","shell.execute_reply":"2024-03-13T06:35:03.871822Z","shell.execute_reply.started":"2024-03-13T06:34:59.240105Z"},"trusted":true},"outputs":[],"source":["# files: 'business', 'tip', 'user', 'checkin', 'review'\n","file_name = 'business'\n","\n","df = read_data_file(config, file_name='business', verbose=False)\n","\n","df = filter_data(config, df, file_name, drop_cols=['attributes', 'hours'], \n","                 dropna=True, dropinf=True, verbose=False)\n","\n","df, col_descriptor_dict = preprocess_data(config, df, handle_outliers=False, verbose=False)\n","\n"]},{"cell_type":"code","execution_count":149,"metadata":{"execution":{"iopub.execute_input":"2024-03-13T06:35:03.875361Z","iopub.status.busy":"2024-03-13T06:35:03.874699Z","iopub.status.idle":"2024-03-13T06:35:08.164424Z","shell.execute_reply":"2024-03-13T06:35:08.162968Z","shell.execute_reply.started":"2024-03-13T06:35:03.875326Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Reading file: /kaggle/input/yelp-compressed-dataset/business.parquet\n","df.shape:  (150346, 14)\n","Before filtering shape:  (150346, 14)\n","Before dropping columns:  Index(['business_id', 'name', 'address', 'city', 'state', 'postal_code',\n","       'latitude', 'longitude', 'stars', 'review_count', 'is_open',\n","       'attributes', 'categories', 'hours'],\n","      dtype='object')\n","Before dropping columns shape: (150346, 14)\n","After dropping columns:  Index(['business_id', 'name', 'address', 'city', 'state', 'postal_code',\n","       'latitude', 'longitude', 'stars', 'review_count', 'is_open',\n","       'categories'],\n","      dtype='object')\n","After dropping columns shape: (150346, 12)\n","Before dropping NaNs shape: (150346, 12)\n","After dropping NaNs shape: (150243, 12)\n","Before dropping Infs shape: (150243, 12)\n","After dropping Infs shape: (150243, 12)\n","Before filterin out non-restaurant data shape: (150243, 12)\n","After filterin out non-restaurant data shape: (52268, 12)\n","After filtering shape:  (52268, 12)\n","Before removing duplicates shape:  (52268, 12)\n","After removing duplicates shape:  (52268, 12)\n","Before encoding data shape:  (52268, 12)\n","After encoding data shape:  (52268, 12)\n","id_cols: \n","['business_id']\n","------------------------------\n","int_cols: \n","['review_count', 'is_open']\n","------------------------------\n","float_cols: \n","['latitude', 'longitude', 'stars']\n","------------------------------\n","nume_cols: \n","['review_count', 'is_open', 'latitude', 'longitude', 'stars']\n","------------------------------\n","non_nume_cols: \n","['business_id', 'name', 'address', 'city', 'state', 'postal_code', 'categories']\n","------------------------------\n","Before outlier handling shape:  (52268, 12)\n","Dropping outliers...\n","After outlier handling shape:  (51305, 12)\n","scaling data....................\n","Before scaling data shape:  (51305, 12)\n","Using standard scaler for cols: ['latitude', 'longitude', 'stars']\n","After scaling data shape:  (51305, 12)\n"]}],"source":["# files: 'business', 'tip', 'user', 'checkin', 'review'\n","file_name = 'business'\n","\n","df = read_data_file(config, file_name=file_name, verbose=True)\n","\n","df = filter_data(config, df, file_name, drop_cols=['attributes', 'hours'], \n","                 dropna=True, dropinf=True, verbose=True)\n","\n","df = preprocess_data(config, df, encode_dates=True, scale_data=True, handle_outliers=True, verbose=True)"]},{"cell_type":"code","execution_count":150,"metadata":{"execution":{"iopub.execute_input":"2024-03-13T06:35:08.168374Z","iopub.status.busy":"2024-03-13T06:35:08.167851Z","iopub.status.idle":"2024-03-13T06:35:16.907416Z","shell.execute_reply":"2024-03-13T06:35:16.905736Z","shell.execute_reply.started":"2024-03-13T06:35:08.168325Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Reading file: /kaggle/input/yelp-compressed-dataset/tip.parquet\n","df.shape:  (908915, 5)\n","Before filtering shape:  (908915, 5)\n","Before dropping NaNs shape: (908915, 5)\n","After dropping NaNs shape: (908915, 5)\n","Before dropping Infs shape: (908915, 5)\n","After dropping Infs shape: (908915, 5)\n","After filtering shape:  (908915, 5)\n","Before removing duplicates shape:  (908915, 5)\n","After removing duplicates shape:  (908848, 5)\n","Before encoding data shape:  (908848, 5)\n","After encoding data shape:  (908848, 5)\n","id_cols: \n","['user_id', 'business_id']\n","------------------------------\n","int_cols: \n","['compliment_count']\n","------------------------------\n","float_cols: \n","[]\n","------------------------------\n","nume_cols: \n","['compliment_count']\n","------------------------------\n","non_nume_cols: \n","['user_id', 'business_id', 'text', 'date']\n","------------------------------\n","Before outlier handling shape:  (908848, 5)\n","Dropping outliers...\n","After outlier handling shape:  (898309, 5)\n","scaling data....................\n","Before scaling data shape:  (898309, 5)\n","Using standard scaler for cols: []\n","After scaling data shape:  (898309, 5)\n"]}],"source":["# files: 'business', 'tip', 'user', 'checkin', 'review'\n","file_name = 'tip'\n","\n","df = read_data_file(config, file_name=file_name, verbose=True)\n","\n","df = filter_data(config, df, file_name, #drop_cols=['attributes', 'hours'], \n","                 dropna=True, dropinf=True, verbose=True)\n","\n","df = preprocess_data(config, df, encode_dates=True, scale_data=True, handle_outliers=True, verbose=True)"]},{"cell_type":"code","execution_count":151,"metadata":{"execution":{"iopub.execute_input":"2024-03-13T06:35:16.910465Z","iopub.status.busy":"2024-03-13T06:35:16.909513Z","iopub.status.idle":"2024-03-13T06:35:21.966135Z","shell.execute_reply":"2024-03-13T06:35:21.964645Z","shell.execute_reply.started":"2024-03-13T06:35:16.910414Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Reading file: /kaggle/input/yelp-compressed-dataset/checkin.parquet\n","df.shape:  (131930, 2)\n","Before filtering shape:  (131930, 2)\n","Before dropping NaNs shape: (131930, 2)\n","After dropping NaNs shape: (131930, 2)\n","Before dropping Infs shape: (131930, 2)\n","After dropping Infs shape: (131930, 2)\n","After filtering shape:  (131930, 2)\n","Before removing duplicates shape:  (131930, 2)\n","After removing duplicates shape:  (131930, 2)\n","id_cols: \n","['business_id']\n","------------------------------\n","int_cols: \n","[]\n","------------------------------\n","float_cols: \n","[]\n","------------------------------\n","nume_cols: \n","[]\n","------------------------------\n","non_nume_cols: \n","['business_id', 'date']\n","------------------------------\n","Before outlier handling shape:  (131930, 2)\n","Dropping outliers...\n","After outlier handling shape:  (131930, 2)\n","scaling data....................\n","Before scaling data shape:  (131930, 2)\n","Using standard scaler for cols: []\n","After scaling data shape:  (131930, 2)\n"]}],"source":["# files: 'business', 'tip', 'user', 'checkin', 'review'\n","file_name = 'checkin'\n","\n","df = read_data_file(config, file_name=file_name, verbose=True)\n","\n","df = filter_data(config, df, file_name, #drop_cols=['attributes', 'hours'], \n","                 dropna=True, dropinf=True, verbose=True)\n","\n","df = preprocess_data(config, df, encode_dates=False, scale_data=True, handle_outliers=True, verbose=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-13T06:35:21.968218Z","iopub.status.busy":"2024-03-13T06:35:21.967780Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Reading file: /kaggle/input/yelp-compressed-dataset/user.parquet\n","df.shape:  (1987897, 22)\n","Before filtering shape:  (1987897, 22)\n","Before dropping columns:  Index(['user_id', 'name', 'review_count', 'yelping_since', 'useful', 'funny',\n","       'cool', 'elite', 'friends', 'fans', 'average_stars', 'compliment_hot',\n","       'compliment_more', 'compliment_profile', 'compliment_cute',\n","       'compliment_list', 'compliment_note', 'compliment_plain',\n","       'compliment_cool', 'compliment_funny', 'compliment_writer',\n","       'compliment_photos'],\n","      dtype='object')\n","Before dropping columns shape: (1987897, 22)\n","After dropping columns:  Index(['user_id', 'name', 'review_count', 'yelping_since', 'useful', 'funny',\n","       'cool', 'elite', 'fans', 'average_stars', 'compliment_hot',\n","       'compliment_more', 'compliment_profile', 'compliment_cute',\n","       'compliment_list', 'compliment_note', 'compliment_plain',\n","       'compliment_cool', 'compliment_funny', 'compliment_writer',\n","       'compliment_photos'],\n","      dtype='object')\n","After dropping columns shape: (1987897, 21)\n","Before dropping NaNs shape: (1987897, 21)\n","After dropping NaNs shape: (1987897, 21)\n","Before dropping Infs shape: (1987897, 21)\n"]}],"source":["# files: 'business', 'tip', 'user', 'checkin', 'review'\n","file_name = 'user'\n","\n","df = read_data_file(config, file_name=file_name, verbose=True)\n","\n","df = filter_data(config, df, file_name, drop_cols=['friends'], \n","                 dropna=True, dropinf=True, verbose=True)\n","\n","df = preprocess_data(config, df, encode_dates=False, scale_data=True, \n","                     handle_outliers=False, verbose=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4491494,"sourceId":7695652,"sourceType":"datasetVersion"}],"dockerImageVersionId":30664,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"undefined.undefined.undefined"}},"nbformat":4,"nbformat_minor":4}
