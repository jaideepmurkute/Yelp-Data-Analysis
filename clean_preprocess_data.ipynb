{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7695652,"sourceType":"datasetVersion","datasetId":4491494}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport json\nimport gc\n\nimport numpy as np \nimport pandas as pd \nimport pyarrow as pa\nimport pyarrow.parquet as pq\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n\n# -------------------------------------------------------\n\nconfig = {\n            'data_dir': '/kaggle/input/yelp-compressed-dataset',\n        }\n\nprint(config)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-13T06:34:59.135539Z","iopub.execute_input":"2024-03-13T06:34:59.136558Z","iopub.status.idle":"2024-03-13T06:34:59.146076Z","shell.execute_reply.started":"2024-03-13T06:34:59.136497Z","shell.execute_reply":"2024-03-13T06:34:59.144501Z"},"trusted":true},"execution_count":146,"outputs":[{"name":"stdout","text":"{'data_dir': '/kaggle/input/yelp-compressed-dataset'}\n","output_type":"stream"}]},{"cell_type":"code","source":"def read_data_file(config, file_name=None, verbose=False):\n    assert file_name is not None\n\n    filepath = os.path.join(config['data_dir'], file_name+'.parquet')\n\n    if verbose: print(f\"Reading file: {filepath}\")\n    df = pq.read_table(filepath).to_pandas()\n    if verbose: print(\"df.shape: \", df.shape)\n\n    return df\n\ndef drop_inf_rows(df):\n    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n    df.dropna(inplace=True)\n    \ndef filter_data(config, df, file_name=None, drop_cols=None, dropna=False, dropinf=False, verbose=False):\n    \n    if verbose: print(\"Before filtering shape: \", df.shape)\n    \n    if drop_cols is not None:\n        if verbose: \n            print(\"Before dropping columns: \", df.columns)\n            print(\"Before dropping columns shape:\", df.shape)\n        df.drop(drop_cols, axis=1, inplace=True)\n        if verbose: \n            print(\"After dropping columns: \", df.columns)\n            print(\"After dropping columns shape:\", df.shape)\n        \n    # Filter out rows with NaN values\n    if dropna:\n        if verbose: print(\"Before dropping NaNs shape:\", df.shape)\n        df.dropna(inplace=True)\n        if verbose: print(\"After dropping NaNs shape:\", df.shape)\n\n    # Filter out rows with inf values\n    if dropinf:\n        if verbose: print(\"Before dropping Infs shape:\", df.shape)\n        df.replace([np.inf, -np.inf], np.nan, inplace=True)\n        df.dropna(inplace=True)\n        if verbose: print(\"After dropping Infs shape:\", df.shape)\n\n    # Filter out ir-relevant data for current analysis\n    assert file_name is not None\n    if file_name == 'business':\n        if verbose: print(\"Before filterin out non-restaurant data shape:\", df.shape)\n        df = df[df['categories'].str.lower().str.contains('restaurants')]\n        if verbose: print(\"After filterin out non-restaurant data shape:\", df.shape)\n    \n    if verbose: print(\"After filtering shape: \", df.shape)\n\n    return df\n\n\ndef handle_outliers_fn(df, cols, mode='drop', threshold=3, verbose=False):\n    if verbose: print(\"Before outlier handling shape: \", df.shape)\n    \n    if mode == 'drop':\n        print(\"Dropping outliers...\")\n        for col in cols:\n            z_scores = (df[col] - df[col].mean()) / df[col].std()\n            df = df[z_scores.abs() < threshold]\n    elif mode == 'clip':\n        print(\"Clipping outliers...\")\n        for col in cols:\n            lower_bound = df[col].mean() - threshold * df[col].std()\n            upper_bound = df[col].mean() + threshold * df[col].std()\n            df[col] = df[col].clip(lower_bound, upper_bound)\n    \n    if verbose: print(\"After outlier handling shape: \", df.shape)\n\n    return df\n\n\ndef remove_duplicates(df, verbose=False):\n    if verbose: print(\"Before removing duplicates shape: \", df.shape)\n    df.drop_duplicates(inplace=True)\n    if verbose: print(\"After removing duplicates shape: \", df.shape)\n    \n    return df\n\n\ndef convert_datetime_column(df, col_name):\n    for idx, dt_str in enumerate(df[col_name]):\n        try:\n            df.loc[idx, col_name] = pd.to_datetime(dt_str, format='%Y-%m-%d %H:%M:%S')  # Or format='mixed'\n        except ValueError:\n            print(f\"Failed to convert datetime string at index {idx}: {dt_str}\")    \n    return df\n\n\ndef encode_dates_fn(df, date_cols=None, verbose=False):\n    if verbose: print(\"Before encoding data shape: \", df.shape)\n    \n    # If the date columns exist; and convert to the datetime format\n    if date_cols is None:\n        # infer date columns based on the names\n        date_cols = [col for col in df.columns if 'date' in col.lower()]\n    if len(date_cols) > 0:\n        for col in date_cols:\n            # format = '%Y-%m-%d %H:%M:%S'\n            # format='mixed'\n            df[col] = df[col].apply(lambda x: x.lstrip(', '))\n            df[col] = df[col].apply(lambda x: x.strip(' '))\n            df[col] = pd.to_datetime(df[col], format='%Y-%m-%d %H:%M:%S')\n            # df = convert_datetime_column(df, col)\n            \n    if verbose: print(\"After encoding data shape: \", df.shape)\n    \n    return df\n\n\ndef get_column_descriptor_dict(config, df, id_cols=None, verbose=False):\n    col_descriptor_dict = dict()\n\n    # create columns type descriptors - int_cols, float_cols, bool_cols, id_cols, numeric_cols, date_cols etc.\n    if id_cols is None:\n        col_descriptor_dict['id_cols'] = [col for col in df.columns if 'id' in col.lower()]\n    col_descriptor_dict['int_cols'] = [col for col in df.columns if 'int' in str(df[col].dtype)]\n    col_descriptor_dict['float_cols'] = [col for col in df.columns if 'float' in str(df[col].dtype)]\n    col_descriptor_dict['nume_cols'] = col_descriptor_dict['int_cols'] + \\\n                                        col_descriptor_dict['float_cols']\n    col_descriptor_dict['non_nume_cols'] = [col for col in df.columns \\\n                                            if col not in col_descriptor_dict['nume_cols']]\n\n    if verbose:\n        for k, v in col_descriptor_dict.items():\n            print(f\"{k}: \")\n            print(f\"{v}\")\n            print(\"-\"*30)\n\n    return col_descriptor_dict\n\n\ndef get_scaler(scaler_type):\n    \"\"\"\n    Returns a scikit-learn scaler object based on the specified scaler type.\n\n    Args:\n        scaler_type (str): Type of scaler. Options are 'standard', 'min_max', 'robust'.  \n\n    Returns:\n        sklearn.preprocessing.scaler: A scikit-learn scaler object.\n    \"\"\"\n\n    if scaler_type == 'standard':\n        scaler = StandardScaler()  # Standardize features (zero mean, unit variance)\n    elif scaler_type == 'min_max':\n        scaler = MinMaxScaler()  # Scale features to a given range (often 0 to 1)\n    elif scaler_type == 'robust':\n        scaler = RobustScaler()  # Scale features using statistics robust to outliers \n    else:\n        raise ValueError(f\"Invalid scaler_type: {scaler_type}\")  # Handle invalid input \n    \n    return scaler\n\n\ndef scale_data_fn(df, scaler_type_cols_map, scaler_type='standard', verbose=False):\n    if verbose: print(\"Before scaling data shape: \", df.shape)\n    \n    for scaler_type, cols in scaler_type_cols_map.items():\n        print(f\"Using {scaler_type} scaler for cols: {cols}\")\n        scaler = get_scaler(scaler_type)\n        for col in cols:\n            df[[col]] = scaler.fit_transform(df[[col]])\n        \n    if verbose: print(\"After scaling data shape: \", df.shape)\n    \n    return df\n\n\ndef preprocess_data(config, df, encode_dates=False, date_cols=None, id_cols=None, \n                    handle_outliers=False, \n                    scale_data=False, cols_scaler_type_map=None, verbose=False):\n    \n    df = remove_duplicates(df, verbose=verbose)\n    \n    if encode_dates:\n        df = encode_dates_fn(df, date_cols, verbose)\n    \n    col_descriptor_dict = get_column_descriptor_dict(config, df, id_cols=id_cols, \\\n                                                     verbose=verbose)\n    \n    if handle_outliers:\n        df = handle_outliers_fn(df, col_descriptor_dict['nume_cols'], \n                                mode='drop', threshold=3, verbose=verbose)\n    \n    scaler_type_cols_map = {'standard': col_descriptor_dict['float_cols']}\n    if scale_data and scaler_type_cols_map:\n        print(\"scaling data....................\")\n        df = scale_data_fn(df, scaler_type_cols_map, verbose=verbose)\n    \n    return df, col_descriptor_dict\n    \n","metadata":{"execution":{"iopub.status.busy":"2024-03-13T06:34:59.185981Z","iopub.execute_input":"2024-03-13T06:34:59.186459Z","iopub.status.idle":"2024-03-13T06:34:59.236219Z","shell.execute_reply.started":"2024-03-13T06:34:59.186426Z","shell.execute_reply":"2024-03-13T06:34:59.234886Z"},"trusted":true},"execution_count":147,"outputs":[]},{"cell_type":"code","source":"# files: 'business', 'tip', 'user', 'checkin', 'review'\nfile_name = 'business'\n\ndf = read_data_file(config, file_name='business', verbose=False)\n\ndf = filter_data(config, df, file_name, drop_cols=['attributes', 'hours'], \n                 dropna=True, dropinf=True, verbose=False)\n\ndf, col_descriptor_dict = preprocess_data(config, df, handle_outliers=False, verbose=False)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-13T06:34:59.238953Z","iopub.execute_input":"2024-03-13T06:34:59.240157Z","iopub.status.idle":"2024-03-13T06:35:03.872913Z","shell.execute_reply.started":"2024-03-13T06:34:59.240105Z","shell.execute_reply":"2024-03-13T06:35:03.871822Z"},"trusted":true},"execution_count":148,"outputs":[]},{"cell_type":"code","source":"# files: 'business', 'tip', 'user', 'checkin', 'review'\nfile_name = 'business'\n\ndf = read_data_file(config, file_name=file_name, verbose=True)\n\ndf = filter_data(config, df, file_name, drop_cols=['attributes', 'hours'], \n                 dropna=True, dropinf=True, verbose=True)\n\ndf = preprocess_data(config, df, encode_dates=True, scale_data=True, handle_outliers=True, verbose=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-13T06:35:03.874699Z","iopub.execute_input":"2024-03-13T06:35:03.875361Z","iopub.status.idle":"2024-03-13T06:35:08.164424Z","shell.execute_reply.started":"2024-03-13T06:35:03.875326Z","shell.execute_reply":"2024-03-13T06:35:08.162968Z"},"trusted":true},"execution_count":149,"outputs":[{"name":"stdout","text":"Reading file: /kaggle/input/yelp-compressed-dataset/business.parquet\ndf.shape:  (150346, 14)\nBefore filtering shape:  (150346, 14)\nBefore dropping columns:  Index(['business_id', 'name', 'address', 'city', 'state', 'postal_code',\n       'latitude', 'longitude', 'stars', 'review_count', 'is_open',\n       'attributes', 'categories', 'hours'],\n      dtype='object')\nBefore dropping columns shape: (150346, 14)\nAfter dropping columns:  Index(['business_id', 'name', 'address', 'city', 'state', 'postal_code',\n       'latitude', 'longitude', 'stars', 'review_count', 'is_open',\n       'categories'],\n      dtype='object')\nAfter dropping columns shape: (150346, 12)\nBefore dropping NaNs shape: (150346, 12)\nAfter dropping NaNs shape: (150243, 12)\nBefore dropping Infs shape: (150243, 12)\nAfter dropping Infs shape: (150243, 12)\nBefore filterin out non-restaurant data shape: (150243, 12)\nAfter filterin out non-restaurant data shape: (52268, 12)\nAfter filtering shape:  (52268, 12)\nBefore removing duplicates shape:  (52268, 12)\nAfter removing duplicates shape:  (52268, 12)\nBefore encoding data shape:  (52268, 12)\nAfter encoding data shape:  (52268, 12)\nid_cols: \n['business_id']\n------------------------------\nint_cols: \n['review_count', 'is_open']\n------------------------------\nfloat_cols: \n['latitude', 'longitude', 'stars']\n------------------------------\nnume_cols: \n['review_count', 'is_open', 'latitude', 'longitude', 'stars']\n------------------------------\nnon_nume_cols: \n['business_id', 'name', 'address', 'city', 'state', 'postal_code', 'categories']\n------------------------------\nBefore outlier handling shape:  (52268, 12)\nDropping outliers...\nAfter outlier handling shape:  (51305, 12)\nscaling data....................\nBefore scaling data shape:  (51305, 12)\nUsing standard scaler for cols: ['latitude', 'longitude', 'stars']\nAfter scaling data shape:  (51305, 12)\n","output_type":"stream"}]},{"cell_type":"code","source":"# files: 'business', 'tip', 'user', 'checkin', 'review'\nfile_name = 'tip'\n\ndf = read_data_file(config, file_name=file_name, verbose=True)\n\ndf = filter_data(config, df, file_name, #drop_cols=['attributes', 'hours'], \n                 dropna=True, dropinf=True, verbose=True)\n\ndf = preprocess_data(config, df, encode_dates=True, scale_data=True, handle_outliers=True, verbose=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-13T06:35:08.167851Z","iopub.execute_input":"2024-03-13T06:35:08.168374Z","iopub.status.idle":"2024-03-13T06:35:16.907416Z","shell.execute_reply.started":"2024-03-13T06:35:08.168325Z","shell.execute_reply":"2024-03-13T06:35:16.905736Z"},"trusted":true},"execution_count":150,"outputs":[{"name":"stdout","text":"Reading file: /kaggle/input/yelp-compressed-dataset/tip.parquet\ndf.shape:  (908915, 5)\nBefore filtering shape:  (908915, 5)\nBefore dropping NaNs shape: (908915, 5)\nAfter dropping NaNs shape: (908915, 5)\nBefore dropping Infs shape: (908915, 5)\nAfter dropping Infs shape: (908915, 5)\nAfter filtering shape:  (908915, 5)\nBefore removing duplicates shape:  (908915, 5)\nAfter removing duplicates shape:  (908848, 5)\nBefore encoding data shape:  (908848, 5)\nAfter encoding data shape:  (908848, 5)\nid_cols: \n['user_id', 'business_id']\n------------------------------\nint_cols: \n['compliment_count']\n------------------------------\nfloat_cols: \n[]\n------------------------------\nnume_cols: \n['compliment_count']\n------------------------------\nnon_nume_cols: \n['user_id', 'business_id', 'text', 'date']\n------------------------------\nBefore outlier handling shape:  (908848, 5)\nDropping outliers...\nAfter outlier handling shape:  (898309, 5)\nscaling data....................\nBefore scaling data shape:  (898309, 5)\nUsing standard scaler for cols: []\nAfter scaling data shape:  (898309, 5)\n","output_type":"stream"}]},{"cell_type":"code","source":"# files: 'business', 'tip', 'user', 'checkin', 'review'\nfile_name = 'checkin'\n\ndf = read_data_file(config, file_name=file_name, verbose=True)\n\ndf = filter_data(config, df, file_name, #drop_cols=['attributes', 'hours'], \n                 dropna=True, dropinf=True, verbose=True)\n\ndf = preprocess_data(config, df, encode_dates=False, scale_data=True, handle_outliers=True, verbose=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-13T06:35:16.909513Z","iopub.execute_input":"2024-03-13T06:35:16.910465Z","iopub.status.idle":"2024-03-13T06:35:21.966135Z","shell.execute_reply.started":"2024-03-13T06:35:16.910414Z","shell.execute_reply":"2024-03-13T06:35:21.964645Z"},"trusted":true},"execution_count":151,"outputs":[{"name":"stdout","text":"Reading file: /kaggle/input/yelp-compressed-dataset/checkin.parquet\ndf.shape:  (131930, 2)\nBefore filtering shape:  (131930, 2)\nBefore dropping NaNs shape: (131930, 2)\nAfter dropping NaNs shape: (131930, 2)\nBefore dropping Infs shape: (131930, 2)\nAfter dropping Infs shape: (131930, 2)\nAfter filtering shape:  (131930, 2)\nBefore removing duplicates shape:  (131930, 2)\nAfter removing duplicates shape:  (131930, 2)\nid_cols: \n['business_id']\n------------------------------\nint_cols: \n[]\n------------------------------\nfloat_cols: \n[]\n------------------------------\nnume_cols: \n[]\n------------------------------\nnon_nume_cols: \n['business_id', 'date']\n------------------------------\nBefore outlier handling shape:  (131930, 2)\nDropping outliers...\nAfter outlier handling shape:  (131930, 2)\nscaling data....................\nBefore scaling data shape:  (131930, 2)\nUsing standard scaler for cols: []\nAfter scaling data shape:  (131930, 2)\n","output_type":"stream"}]},{"cell_type":"code","source":"# files: 'business', 'tip', 'user', 'checkin', 'review'\nfile_name = 'user'\n\ndf = read_data_file(config, file_name=file_name, verbose=True)\n\ndf = filter_data(config, df, file_name, drop_cols=['friends'], \n                 dropna=True, dropinf=True, verbose=True)\n\ndf = preprocess_data(config, df, encode_dates=False, scale_data=True, \n                     handle_outliers=False, verbose=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-13T06:35:21.967780Z","iopub.execute_input":"2024-03-13T06:35:21.968218Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Reading file: /kaggle/input/yelp-compressed-dataset/user.parquet\ndf.shape:  (1987897, 22)\nBefore filtering shape:  (1987897, 22)\nBefore dropping columns:  Index(['user_id', 'name', 'review_count', 'yelping_since', 'useful', 'funny',\n       'cool', 'elite', 'friends', 'fans', 'average_stars', 'compliment_hot',\n       'compliment_more', 'compliment_profile', 'compliment_cute',\n       'compliment_list', 'compliment_note', 'compliment_plain',\n       'compliment_cool', 'compliment_funny', 'compliment_writer',\n       'compliment_photos'],\n      dtype='object')\nBefore dropping columns shape: (1987897, 22)\nAfter dropping columns:  Index(['user_id', 'name', 'review_count', 'yelping_since', 'useful', 'funny',\n       'cool', 'elite', 'fans', 'average_stars', 'compliment_hot',\n       'compliment_more', 'compliment_profile', 'compliment_cute',\n       'compliment_list', 'compliment_note', 'compliment_plain',\n       'compliment_cool', 'compliment_funny', 'compliment_writer',\n       'compliment_photos'],\n      dtype='object')\nAfter dropping columns shape: (1987897, 21)\nBefore dropping NaNs shape: (1987897, 21)\nAfter dropping NaNs shape: (1987897, 21)\nBefore dropping Infs shape: (1987897, 21)\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}