{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import json\n",
    "\n",
    "torch.set_num_threads(4)\n",
    "np.set_printoptions(linewidth=200) # default 75\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from CFG import Config\n",
    "cfg_ref = Config()\n",
    "config = cfg_ref.get_config()\n",
    "# config = {'data_dir': '/kaggle/input/yelp-compressed-dataset'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "business_df = pq.read_table(os.path.join(config['data_dir'], 'business.parquet')).to_pandas()\n",
    "restaurants_df = business_df[business_df['categories'].apply(\\\n",
    "                    lambda x: 'Restaurants' in x if x is not None else False)]\n",
    "print(\"restaurants_df.shape\", restaurants_df.shape)\n",
    "del business_df\n",
    "\n",
    "# users_df = pq.read_table(os.path.join(config['data_dir'], 'user.parquet')).to_pandas()\n",
    "# checkin_df = pq.read_table(os.path.join(config['data_dir'], 'checkin.parquet')).to_pandas()\n",
    "\n",
    "review_df_sample = pd.read_csv(os.path.join(config['data_dir'], 'review_df_sample.csv'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restanrant_name = 'Willie Mae\\'s Scotch House'\n",
    "# city = 'New Orleans'\n",
    "\n",
    "# curr_rest_business_id = restaurants_df[(restaurants_df['name'] == restanrant_name) & \n",
    "#                             (restaurants_df['city'] == city)].business_id\n",
    "# curr_rest_business_id = curr_rest_business_id.values[0]\n",
    "\n",
    "curr_rest_business_id = '-Tskf8WK17rb3ZfeFuRSWA'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_restau_review_df = review_df_sample[review_df_sample['business_id'] == curr_rest_business_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_sentiment_classifier_pipeline():\n",
    "    sentiment_classifier_pipeline = pipeline(\n",
    "            \"sentiment-analysis\", \n",
    "            model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "            device=-1, # Force CPU usage (0 = index of first CPU)\n",
    "            truncation=True,\n",
    "            max_length=512, # Increase the max length to 512\n",
    "        )\n",
    "    \n",
    "    return sentiment_classifier_pipeline\n",
    "\n",
    "\n",
    "def classify_sentiment(df, classifier_pipeline):\n",
    "    results = classifier_pipeline(df['text'].tolist())  \n",
    "    df['sentiment_label'] = [1 if result['label'] == 'POSITIVE' else 0 for result in results]\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vitthal\\AppData\\Local\\Temp\\ipykernel_8596\\2941301122.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['sentiment_label'] = [1 if result['label'] == 'POSITIVE' else 0 for result in results]\n"
     ]
    }
   ],
   "source": [
    "dummy_input_df = pd.DataFrame({\n",
    "    'text': ['This product is amazing!', 'Terrible customer service.', 'It works okay.']\n",
    "})\n",
    "\n",
    "sent_classifier_pipeline = get_sentiment_classifier_pipeline()\n",
    "curr_restau_review_df = classify_sentiment(curr_restau_review_df, sent_classifier_pipeline)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive reviews: 2\n",
      "Number of negative reviews: 1\n"
     ]
    }
   ],
   "source": [
    "curr_restau_pos_reviews = curr_restau_review_df[curr_restau_review_df['sentiment_label'] == 1]['text'].to_frame()\n",
    "curr_restau_neg_reviews = curr_restau_review_df[curr_restau_review_df['sentiment_label'] == 0]['text'].to_frame()\n",
    "\n",
    "print(\"Number of positive reviews:\", len(curr_restau_pos_reviews))\n",
    "print(\"Number of negative reviews:\", len(curr_restau_neg_reviews))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_text_summarizer_pipeline():\n",
    "    # Create the summarization pipeline\n",
    "    summarizer = pipeline(\n",
    "            \"summarization\",\n",
    "            model=\"t5-base\",\n",
    "            device=-1,  # Force CPU usage (0 = index of first CPU)\n",
    "            truncation=True,\n",
    "            max_length=512,  # Set the maximum output length\n",
    "        )\n",
    "    \n",
    "    return summarizer\n",
    "\n",
    "# Function to summarize the reviews\n",
    "def summarize_reviews(df, summarizer):\n",
    "    # Get the list of reviews\n",
    "    reviews = df['text'].tolist()\n",
    "    \n",
    "    # Summarize the reviews\n",
    "    summaries = summarizer(reviews, truncation=True, max_length=512)\n",
    "    \n",
    "    # Add the summaries to the dataframe\n",
    "    df['summary'] = [summary['summary_text'] for summary in summaries]\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_review_chunks(reviews, num_chunks=5, each_chunk_reviews=10):\n",
    "    # Create n chunks of k reviews each - we want n revuew summaries \n",
    "    review_chunks = []\n",
    "    for i in range(num_chunks):\n",
    "        # Generate 10 random indices\n",
    "        random_indices = np.random.choice(reviews.shape[0], size=min(reviews.shape[0], \n",
    "                        each_chunk_reviews), replace=False)\n",
    "\n",
    "        # Extract 'text' field from each row using the random indices\n",
    "        selected_texts = reviews.iloc[random_indices]['text'].tolist()\n",
    "\n",
    "        # Merge all the text rows\n",
    "        merged_text = ''.join(selected_texts)\n",
    "\n",
    "        review_chunks.append(merged_text)\n",
    "\n",
    "    return pd.DataFrame({'text': review_chunks})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_chunks = 5  # How many review chunks, each of size 'each_chunk_reviews', to summaize\n",
    "each_chunk_reviews = 10  # Number of reviews in each chunk\n",
    "\n",
    "positive_reviews_chunk = get_review_chunks(curr_restau_pos_reviews, num_chunks, each_chunk_reviews)\n",
    "negative_reviews_chunk = get_review_chunks(curr_restau_pos_reviews, num_chunks, each_chunk_reviews)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vitthal\\anaconda3\\envs\\quant_trading_env_1\\Lib\\site-packages\\transformers\\models\\t5\\tokenization_t5_fast.py:160: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "summarizer_pipeline = get_text_summarizer_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing positive reviews...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 512, but your input_length is only 266. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=133)\n",
      "Your max_length is set to 512, but your input_length is only 266. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=133)\n",
      "Your max_length is set to 512, but your input_length is only 266. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=133)\n",
      "Your max_length is set to 512, but your input_length is only 268. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=134)\n",
      "Your max_length is set to 512, but your input_length is only 268. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=134)\n",
      "Your max_length is set to 512, but your input_length is only 268. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=134)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing negative reviews...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 512, but your input_length is only 268. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=134)\n",
      "Your max_length is set to 512, but your input_length is only 268. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=134)\n",
      "Your max_length is set to 512, but your input_length is only 266. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=133)\n",
      "Your max_length is set to 512, but your input_length is only 268. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=134)\n"
     ]
    }
   ],
   "source": [
    "print(\"Summarizing positive reviews...\")\n",
    "positive_review_summaries = summarize_reviews(positive_reviews_chunk, summarizer_pipeline)\n",
    "print(\"Summarizing negative reviews...\")\n",
    "negative_review_summaries = summarize_reviews(negative_reviews_chunk, summarizer_pipeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the service was amazing, from the bus boy, to the server, and the person doing our dessert . the gumbo was delicious, and it was a beautiful place to duck in and escape the snow after visiting the statue down the street .the service was amazing, from the bus boy, to the server, and the person doing our dessert . the gumbo was delicious, and it was a beautiful place to duck in and escape the snow after visiting the statue down the street .the service was amazing, from the bus boy, to the server, and the person doing our dessert . the gumbo was delicious, and it was a beautiful place to duck in and escape the snow after visiting the statue down the street .ignatius j Reilly statue was down the street . we arrived around 8pm on a friday night . the wait was fairly short: 10-15 minutes .ignatius j Reilly statue was down the street . we arrived around 8pm on a friday night . the wait was fairly short: 10-15 minutes .'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(positive_review_summaries.summary.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ignatius j Reilly statue was down the street . we arrived around 8pm on a friday night . the wait was fairly short: 10-15 minutes .ignatius j Reilly statue was down the street . we arrived around 8pm on a friday night . the wait was fairly short: 10-15 minutes .ignatius j Reilly statue was down the street . we arrived around 8pm on a friday night . the wait was fairly short: 10-15 minutes .the service was amazing, from the bus boy, to the server, and the person doing our dessert . the gumbo was delicious, and it was a beautiful place to duck in and escape the snow after visiting the statue down the street .ignatius j Reilly statue was down the street . we arrived around 8pm on a friday night . the wait was fairly short: 10-15 minutes .'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(negative_review_summaries.summary.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quant_trading_env_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
